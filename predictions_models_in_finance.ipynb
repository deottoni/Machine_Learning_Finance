{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING DIFFERENT MODELS TO PREDICT INDEX PERFORMANCE\n",
    "\n",
    " i. LSTM\n",
    " ii. ARIMA\n",
    " iii. BROWNIAN MOTION\n",
    " iv. PROPHET\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROPOSALS OF INVESTMENTS PORTFOLIOS\n",
    "\n",
    " i. Markowitz’s Minimum-Variance Portfolio (MVP)\n",
    " ii. Traditional risk parity’s Inverse-Variance Portfolio (IVP)\n",
    " iii. Hierarchical Risk Parity (HRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fbprophet\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/fb/ad98d46773929079657706e6b2b6e366ba6c282bc2397d8f9b0ea8e5614c/fbprophet-0.5.tar.gz (49kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 2.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Cython>=0.22 in /Users/aottoni/anaconda3/lib/python3.7/site-packages (from fbprophet) (0.29.6)\n",
      "Collecting pystan>=2.14 (from fbprophet)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/8b/9b48058afd41c41ba704eacae7ee2186a16f2cb6e22970fc97deca8bd6df/pystan-2.19.0.0-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 698kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.10.0 in /Users/aottoni/anaconda3/lib/python3.7/site-packages (from fbprophet) (1.16.2)\n",
      "Requirement already satisfied: pandas>=0.23.4 in /Users/aottoni/anaconda3/lib/python3.7/site-packages (from fbprophet) (0.24.2)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /Users/aottoni/anaconda3/lib/python3.7/site-packages (from fbprophet) (3.0.3)\n",
      "Collecting lunardate>=0.1.5 (from fbprophet)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/7e/377a3cbba646ec0cf79433ef858881d809a3b87eb887b0901cb83c66a758/lunardate-0.2.0-py3-none-any.whl\n",
      "Collecting convertdate>=2.1.2 (from fbprophet)\n",
      "  Downloading https://files.pythonhosted.org/packages/74/83/d0fa07078f4d4ae473a89d7d521aafc66d82641ea0af0ef04a47052e8f17/convertdate-2.1.3-py2.py3-none-any.whl\n",
      "Collecting holidays>=0.9.5 (from fbprophet)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/09/c882bee98acfa310933b654697405260ec7657c78430a14e785ef0f1314b/holidays-0.9.10.tar.gz (73kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 7.9MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting setuptools-git>=1.2 (from fbprophet)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/97/dd99fa9c0d9627a7b3c103a00f1566d8193aca8d473884ed258cca82b06f/setuptools_git-1.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /Users/aottoni/anaconda3/lib/python3.7/site-packages (from pandas>=0.23.4->fbprophet) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/aottoni/anaconda3/lib/python3.7/site-packages (from pandas>=0.23.4->fbprophet) (2018.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/aottoni/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.0.0->fbprophet) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/aottoni/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.0.0->fbprophet) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/aottoni/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.0.0->fbprophet) (2.3.1)\n",
      "Collecting ephem<3.8,>=3.7.5.3 (from convertdate>=2.1.2->fbprophet)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/2c/9e1a815add6c222a0d4bf7c644e095471a934a39bc90c201f9550a8f7f14/ephem-3.7.6.0.tar.gz (739kB)\n",
      "\u001b[K    100% |████████████████████████████████| 747kB 7.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/aottoni/anaconda3/lib/python3.7/site-packages (from holidays>=0.9.5->fbprophet) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /Users/aottoni/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->fbprophet) (40.8.0)\n",
      "Building wheels for collected packages: fbprophet, holidays, ephem\n",
      "  Building wheel for fbprophet (setup.py) ... \u001b[?25lerror\n",
      "  Complete output from command /Users/aottoni/anaconda3/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/4k/xzn14cxj5z7807rnft1z2rbh0000gn/T/pip-install-vcok7ofz/fbprophet/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /private/var/folders/4k/xzn14cxj5z7807rnft1z2rbh0000gn/T/pip-wheel-jafuzgxx --python-tag cp37:\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib\n",
      "  creating build/lib/fbprophet\n",
      "  creating build/lib/fbprophet/stan_model\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/private/var/folders/4k/xzn14cxj5z7807rnft1z2rbh0000gn/T/pip-install-vcok7ofz/fbprophet/setup.py\", line 120, in <module>\n",
      "      \"\"\"\n",
      "    File \"/Users/aottoni/anaconda3/lib/python3.7/site-packages/setuptools/__init__.py\", line 145, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"/Users/aottoni/anaconda3/lib/python3.7/distutils/core.py\", line 148, in setup\n",
      "      dist.run_commands()\n",
      "    File \"/Users/aottoni/anaconda3/lib/python3.7/distutils/dist.py\", line 966, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"/Users/aottoni/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/Users/aottoni/anaconda3/lib/python3.7/site-packages/wheel/bdist_wheel.py\", line 192, in run\n",
      "      self.run_command('build')\n",
      "    File \"/Users/aottoni/anaconda3/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/Users/aottoni/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/Users/aottoni/anaconda3/lib/python3.7/distutils/command/build.py\", line 135, in run\n",
      "      self.run_command(cmd_name)\n",
      "    File \"/Users/aottoni/anaconda3/lib/python3.7/distutils/cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"/Users/aottoni/anaconda3/lib/python3.7/distutils/dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"/private/var/folders/4k/xzn14cxj5z7807rnft1z2rbh0000gn/T/pip-install-vcok7ofz/fbprophet/setup.py\", line 44, in run\n",
      "      build_stan_model(target_dir)\n",
      "    File \"/private/var/folders/4k/xzn14cxj5z7807rnft1z2rbh0000gn/T/pip-install-vcok7ofz/fbprophet/setup.py\", line 27, in build_stan_model\n",
      "      from pystan import StanModel\n",
      "  ModuleNotFoundError: No module named 'pystan'\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for fbprophet\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for fbprophet\n",
      "  Building wheel for holidays (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/aottoni/Library/Caches/pip/wheels/2d/cc/52/784fca01997448402695e3d0356d2f9c814d545bbcdef45d26\n",
      "  Building wheel for ephem (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/aottoni/Library/Caches/pip/wheels/c2/19/af/23739730a62bb43acb1ff256c9234b0f1a94c4666c140a3c1d\n",
      "Successfully built holidays ephem\n",
      "Failed to build fbprophet\n",
      "Installing collected packages: pystan, lunardate, ephem, convertdate, holidays, setuptools-git, fbprophet\n",
      "  Running setup.py install for fbprophet ... \u001b[?25lerror\n",
      "    Complete output from command /Users/aottoni/anaconda3/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/4k/xzn14cxj5z7807rnft1z2rbh0000gn/T/pip-install-vcok7ofz/fbprophet/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /private/var/folders/4k/xzn14cxj5z7807rnft1z2rbh0000gn/T/pip-record-h4xgvsm8/install-record.txt --single-version-externally-managed --compile:\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib\n",
      "    creating build/lib/fbprophet\n",
      "    creating build/lib/fbprophet/stan_model\n",
      "    INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_861b75c6337e237650a61ae58c4385ef NOW.\n",
      "    error: command 'gcc' failed with exit status 1\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"/Users/aottoni/anaconda3/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/4k/xzn14cxj5z7807rnft1z2rbh0000gn/T/pip-install-vcok7ofz/fbprophet/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /private/var/folders/4k/xzn14cxj5z7807rnft1z2rbh0000gn/T/pip-record-h4xgvsm8/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /private/var/folders/4k/xzn14cxj5z7807rnft1z2rbh0000gn/T/pip-install-vcok7ofz/fbprophet/\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install fbprophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e696ff92358d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfbprophet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "#Defining libraries and dependencies\n",
    "import warnings\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import quandl \n",
    "import fbprophet\n",
    "import datetime\n",
    "from datetime import date\n",
    "import pandas_datareader.data as web\n",
    "from pandas_datareader import data as pdr\n",
    "# import fix_yahoo_finance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "quandl.ApiConfig.api_key = \"qrtyy6k9BVDXhBK7AYo4\"\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.optimizers import SGD\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from datetime import date\n",
    "from matplotlib import pyplot as plt\n",
    "import cvxopt as opt\n",
    "from cvxopt import blas, solvers\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import ffn\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Data\n",
    "\n",
    "todays_day = date.today() \n",
    "start = datetime.datetime(2010, 1, 1)\n",
    "end = todays_day\n",
    "\n",
    "\n",
    "df_aapl = pdr.get_data_yahoo(\"AAPL\", start=start, end=end)['Adj Close']\n",
    "df=pd.DataFrame(df_aapl)\n",
    "df.plot(title='AAPL Adj. Closing Price',figsize=(20,10))\n",
    "df.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Time Series Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "# Give X look_back time what will tomorrows prediction be?\n",
    "def create_dataset(dataset, look_back=8):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into X=t and Y=t+1\n",
    "import numpy as np\n",
    "look_back = 10\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Andry\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.1)\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(1, input_shape=(look_back, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Andry\\Anaconda3\\envs\\PythonData\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " - 5s - loss: 0.0069\n",
      "Epoch 2/50\n",
      " - 5s - loss: 1.0861e-04\n",
      "Epoch 3/50\n",
      " - 6s - loss: 1.0587e-04\n",
      "Epoch 4/50\n",
      " - 5s - loss: 1.0592e-04\n",
      "Epoch 5/50\n",
      " - 5s - loss: 1.0425e-04\n",
      "Epoch 6/50\n",
      " - 6s - loss: 9.9500e-05\n",
      "Epoch 7/50\n",
      " - 8s - loss: 9.8797e-05\n",
      "Epoch 8/50\n",
      " - 7s - loss: 9.6479e-05\n",
      "Epoch 9/50\n",
      " - 6s - loss: 9.6815e-05\n",
      "Epoch 10/50\n",
      " - 6s - loss: 9.6586e-05\n",
      "Epoch 11/50\n",
      " - 6s - loss: 9.6803e-05\n",
      "Epoch 12/50\n",
      " - 5s - loss: 9.4298e-05\n",
      "Epoch 13/50\n",
      " - 5s - loss: 9.1685e-05\n",
      "Epoch 14/50\n",
      " - 5s - loss: 9.2814e-05\n",
      "Epoch 15/50\n",
      " - 6s - loss: 9.1356e-05\n",
      "Epoch 16/50\n",
      " - 6s - loss: 8.9720e-05\n",
      "Epoch 17/50\n",
      " - 5s - loss: 9.1589e-05\n",
      "Epoch 18/50\n",
      " - 5s - loss: 8.9646e-05\n",
      "Epoch 19/50\n",
      " - 5s - loss: 8.6387e-05\n",
      "Epoch 20/50\n",
      " - 6s - loss: 8.7732e-05\n",
      "Epoch 21/50\n",
      " - 5s - loss: 8.7983e-05\n",
      "Epoch 22/50\n",
      " - 6s - loss: 8.3190e-05\n",
      "Epoch 23/50\n",
      " - 5s - loss: 8.5310e-05\n",
      "Epoch 24/50\n",
      " - 6s - loss: 8.2856e-05\n",
      "Epoch 25/50\n",
      " - 6s - loss: 8.2416e-05\n",
      "Epoch 26/50\n",
      " - 7s - loss: 8.2107e-05\n",
      "Epoch 27/50\n",
      " - 6s - loss: 8.2165e-05\n",
      "Epoch 28/50\n",
      " - 6s - loss: 8.0386e-05\n",
      "Epoch 29/50\n",
      " - 6s - loss: 8.2023e-05\n",
      "Epoch 30/50\n",
      " - 6s - loss: 8.1835e-05\n",
      "Epoch 31/50\n",
      " - 6s - loss: 8.1241e-05\n",
      "Epoch 32/50\n",
      " - 6s - loss: 7.8709e-05\n",
      "Epoch 33/50\n",
      " - 5s - loss: 7.9479e-05\n",
      "Epoch 34/50\n",
      " - 6s - loss: 7.7190e-05\n",
      "Epoch 35/50\n",
      " - 6s - loss: 7.7000e-05\n",
      "Epoch 36/50\n",
      " - 5s - loss: 7.7794e-05\n",
      "Epoch 37/50\n",
      " - 5s - loss: 7.6310e-05\n",
      "Epoch 38/50\n",
      " - 5s - loss: 7.7704e-05\n",
      "Epoch 39/50\n",
      " - 5s - loss: 7.6759e-05\n",
      "Epoch 40/50\n",
      " - 5s - loss: 7.6320e-05\n",
      "Epoch 41/50\n",
      " - 5s - loss: 7.4989e-05\n",
      "Epoch 42/50\n",
      " - 7s - loss: 7.5456e-05\n",
      "Epoch 43/50\n",
      " - 6s - loss: 7.3134e-05\n",
      "Epoch 44/50\n",
      " - 6s - loss: 7.4043e-05\n",
      "Epoch 45/50\n",
      " - 5s - loss: 7.3607e-05\n",
      "Epoch 46/50\n",
      " - 6s - loss: 7.2308e-05\n",
      "Epoch 47/50\n",
      " - 6s - loss: 7.1112e-05\n",
      "Epoch 48/50\n",
      " - 5s - loss: 7.1873e-05\n",
      "Epoch 49/50\n",
      " - 6s - loss: 7.2467e-05\n",
      "Epoch 50/50\n",
      " - 5s - loss: 7.2786e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2cc93d237b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=50, batch_size=1, verbose=2, callbacks=[EarlyStopping(monitor='loss', patience=20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"aapl_ac.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"aapl_ac.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_10 = [[100,101,110,105,100,101,110,105,120,130]]\n",
    "processed_fake_10 = scaler.transform(fake_10)\n",
    "processed_fake_10.reshape(1,10,1)\n",
    "output = model.predict(processed_fake_10.reshape(1,10,1))\n",
    "scaler.inverse_transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "LSTM_MSE  = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (LSTM_MSE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.title(\"AAPL LSTM FORECAST\")\n",
    "plt.savefig(\"Images/AAPL_LSTM_img.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive integrated moving average (ARIMA)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Arima Brief Explanation\n",
    "#https://people.duke.edu/~rnau/411arim.htm\n",
    "\n",
    "# ARIMA(p,d,q) forecasting equation: ARIMA models are, in theory, the most general class of models for forecasting a time series which can be made to be “stationary” by differencing (if necessary), perhaps in conjunction with nonlinear transformations such as logging or deflating (if necessary). A random variable that is a time series is stationary if its statistical properties are all constant over time.  A stationary series has no trend, its variations around its mean have a constant amplitude, and it wiggles in a consistent fashion, i.e., its short-term random time patterns always look the same in a statistical sense.  The latter condition means that its autocorrelations (correlations with its own prior deviations from the mean) remain constant over time, or equivalently, that its power spectrum remains constant over time.  A random variable of this form can be viewed (as usual) as a combination of signal and noise, and the signal (if one is apparent) could be a pattern of fast or slow mean reversion, or sinusoidal oscillation, or rapid alternation in sign, and it could also have a seasonal component.  An ARIMA model can be viewed as a “filter” that tries to separate the signal from the noise, and the signal is then extrapolated into the future to obtain forecasts.\n",
    "# The ARIMA forecasting equation for a stationary time series is a linear (i.e., regression-type) equation in which the predictors consist of lags of the dependent variable and/or lags of the forecast errors.  That is:\n",
    "# Predicted value of Y = a constant and/or a weighted sum of one or more recent values of Y and/or a weighted sum of one or more recent values of the errors.\n",
    "# If the predictors consist only of lagged values of Y, it is a pure autoregressive (“self-regressed”) model, which is just a special case of a regression model and which could be fitted with standard regression software.  For example, a first-order autoregressive (“AR(1)”) model for Y is a simple regression model in which the independent variable is just Y lagged by one period (LAG(Y,1) in Statgraphics or Y_LAG1 in RegressIt).  If some of the predictors are lags of the errors, an ARIMA model it is NOT a linear regression model, because there is no way to specify “last period’s error” as an independent variable:  the errors must be computed on a period-to-period basis when the model is fitted to the data.  From a technical standpoint, the problem with using lagged errors as predictors is that the model’s predictions are not linear functions of the coefficients, even though they are linear functions of the past data.  So, coefficients in ARIMA models that include lagged errors must be estimated by nonlinear optimization methods (“hill-climbing”) rather than by just solving a system of equations.\n",
    "\n",
    "\n",
    "# The acronym ARIMA stands for Auto-Regressive Integrated Moving Average. Lags of the stationarized series in the forecasting equation are called \"autoregressive\" terms, lags of the forecast errors are called \"moving average\" terms, and a time series which needs to be differenced to be made stationary is said to be an \"integrated\" version of a stationary series. Random-walk and random-trend models, autoregressive models, and exponential smoothing models are all special cases of ARIMA models.\n",
    "# A nonseasonal ARIMA model is classified as an \"ARIMA(p,d,q)\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the Data\n",
    "todays_day = date.today() \n",
    "start = datetime.datetime(2010, 1, 1)\n",
    "end = todays_day\n",
    "\n",
    "\n",
    "apple = pdr.get_data_yahoo(\"AAPL\", start=start, end=end)['Adj Close']\n",
    "df=pd.DataFrame(apple)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'MS' string groups the data in buckets by start of the period\n",
    "y=df\n",
    "y = y['Adj Close'].resample('MS').mean()\n",
    "\n",
    "# The term bfill means that we use the value before filling in missing values\n",
    "y = y.fillna(y.bfill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Selection for the ARIMA Time Series Model\n",
    "\n",
    "# p is the number of autoregressive terms,\n",
    "# d is the number of nonseasonal differences needed for stationarity, and\n",
    "# q is the number of lagged forecast errors in the prediction equation\n",
    "\n",
    "# ARIMA(p,d,q) forecasting equation\n",
    "# ARIMA(1,0,0) = first-order autoregressive model\n",
    "# ARIMA(0,1,0) = random walk \n",
    "# ARIMA(1,1,0) = differenced first-order autoregressive model \n",
    "# ARIMA(0,1,1) without constant = simple exponential smoothing \n",
    "# ARIMA(0,1,1) with constant = simple exponential smoothing with growth \n",
    "# ARIMA(0,2,1) or (0,2,2) without constant = linear exponential smoothing \n",
    "# ARIMA(1,1,2) with constant = damped-trend linear exponential smoothing \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the p, d and q parameters to take any value between 0 and 2\n",
    "p = d = q = range(0, 2)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q triplets\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "print('Examples of parameter combinations for Seasonal ARIMA...')\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\n",
    "print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "\n",
    "for param in pdq:\n",
    "    for param_seasonal in seasonal_pdq:\n",
    "        try:\n",
    "            mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                            order=param,\n",
    "                                            seasonal_order=param_seasonal,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=True)\n",
    "\n",
    "            results = mod.fit()\n",
    "\n",
    "            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting an ARIMA Time Series Model\n",
    "mod= sm.tsa.statespace.SARIMAX(y,\n",
    "                                order=(1, 1, 1),\n",
    "                                seasonal_order=(1, 1, 1, 12),\n",
    "                                enforce_stationarity=False,\n",
    "                                enforce_invertibility=False)\n",
    "\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_diagnostics(figsize=(15, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating Forecasts\n",
    "pred = results.get_prediction(start=pd.to_datetime('2015-01-01'), dynamic=False)\n",
    "pred_ci = pred.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = y['2015':].plot(label='observed')\n",
    "pred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7)\n",
    "\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.2)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "# ax.set_ylabel('Index')\n",
    "plt.title(\"AAPL ARIMA FORECAST\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forecasted = pred.predicted_mean\n",
    "y_truth = y['2015-01-01':]\n",
    "\n",
    "# Compute the mean square error\n",
    "ARIMA_MSE1 = ((y_forecasted - y_truth) ** 2).mean()\n",
    "print('The Mean Squared Error is{}'.format(round(ARIMA_MSE1, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A better representation of our true predictive power can be obtained using dynamic \n",
    "pred_dynamic = results.get_prediction(start=pd.to_datetime('2015-01-01'), dynamic=True, full_results=True)\n",
    "pred_dynamic_ci = pred_dynamic.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = y['2010':].plot(label='observed', figsize=(20, 15))\n",
    "pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
    "\n",
    "ax.fill_between(pred_dynamic_ci.index,\n",
    "                pred_dynamic_ci.iloc[:, 0],\n",
    "                pred_dynamic_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "\n",
    "ax.fill_betweenx(ax.get_ylim(), pd.to_datetime('2015-01-01'), y.index[-1],\n",
    "                 alpha=.1, zorder=-1)\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Index')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the predicted and true values of our time series\n",
    "y_forecasted = pred_dynamic.predicted_mean\n",
    "y_truth = y['2015-01-01':]\n",
    "\n",
    "# Compute the mean square error\n",
    "ARIMA_MSE = ((y_forecasted - y_truth) ** 2).mean()\n",
    "print('The Mean Squared Error of our forecasts is {}'.format(round(ARIMA_MSE, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Producing and Visualizing Forecasts\n",
    "# Get forecast # steps ahead in future\n",
    "pred_uc = results.get_forecast(steps=40)\n",
    "\n",
    "# Get confidence intervals of forecasts\n",
    "pred_ci = pred_uc.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = y.plot(label='observed', figsize=(20, 15))\n",
    "pred_uc.predicted_mean.plot(ax=ax, label='Forecast')\n",
    "ax.fill_between(pred_ci.index,\n",
    "                pred_ci.iloc[:, 0],\n",
    "                pred_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Index')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"AAPL ARIMA FORECAST\")\n",
    "plt.savefig(\"Images/AAPL_ARIMA_img.png\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brownian Motion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Brownian motion is a simple continuous stochastic process that is widely used in physics and finance for modeling random behavior that evolves over time. Examples of such behavior are the random movements of a molecule of gas or fluctuations in an asset’s price. Brownian motion was introduced by Robert Brown (1828) who observed in 1827 how particles of pollen suspended in water moved erratically on a microscopic scale. The motion was caused by water molecules randomly buffeting the particle of pollen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Data\n",
    "ts = TimeSeries(key='VW8RXL88BTZWTRN4', output_format='pandas')\n",
    "data, meta_data = ts.get_intraday(symbol='AAPL',interval='1min', outputsize='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can describe it\n",
    "data['4. close']\n",
    "data = data['4. close']\n",
    "df=pd.DataFrame(data)\n",
    "df.tail()\n",
    "\n",
    "#Using Quandl\n",
    "# start = \"2010-01-01\"\n",
    "# end = \"2018-06-30\"\n",
    "# df = quandl.get(\"WIKI/AAPL.4\",start_date=start, end_date=end)\n",
    "# df.plot(figsize=(20,10))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Close = df['4. close']\n",
    "time = np.linspace(1, len(Close), len(Close))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5       \n",
    "N  = 2.**6     # increments\n",
    "\n",
    "def Brownian(seed, N):\n",
    "    \n",
    "    np.random.seed(seed)                         \n",
    "    dt = 1./N                                    # time step\n",
    "    b = np.random.normal(0., 1., int(N))*np.sqrt(dt)  # brownian increments\n",
    "    W = np.cumsum(b)                             # brownian path\n",
    "    return W, b\n",
    "\n",
    "\n",
    "# brownian increments\n",
    "b = Brownian(seed, N)[1]\n",
    "\n",
    "# brownian motion\n",
    "W = Brownian(seed, N)[0]\n",
    "W = np.insert(W, 0, 0.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_return(Close):\n",
    "    returns = []\n",
    "    for i in range(0, len(Close)-1):\n",
    "        today = Close[i+1]\n",
    "        yesterday = Close[i]\n",
    "        daily_return = (today - yesterday)/yesterday\n",
    "        returns.append(daily_return)\n",
    "    return returns\n",
    "\n",
    "returns = daily_return(Close)\n",
    "\n",
    "mu = np.mean(returns)*252.           # drift coefficient\n",
    "sig = np.std(returns)*np.sqrt(252.)  # diffusion coefficient\n",
    "\n",
    "print(mu, sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBM Exact Solution\n",
    "\n",
    "# Parameters\n",
    "#\n",
    "# So:     initial stock price\n",
    "# mu:     returns (drift coefficient)\n",
    "# sigma:  volatility (diffusion coefficient)\n",
    "# W:      brownian motion\n",
    "# T:      time period\n",
    "# N:      number of increments\n",
    "\n",
    "def GBM(So, mu, sigma, W, T, N):    \n",
    "    t = np.linspace(0.,1.,N+1)\n",
    "    S = []\n",
    "    S.append(So)\n",
    "    for i in range(1,int(N+1)):\n",
    "        drift = (mu - 0.5 * sigma**2) * t[i]\n",
    "        diffusion = sigma * W[i-1]\n",
    "        S_temp = So*np.exp(drift + diffusion)\n",
    "        S.append(S_temp)\n",
    "    return S, t\n",
    "\n",
    "seed = 22\n",
    "So = Close[0]            # Initial Stock price \n",
    "W = Brownian(seed, N)[0]\n",
    "T = 1.\n",
    "N = 2.**6\n",
    "\n",
    "soln = GBM(So, mu, sig, W, T, N)[0]    # Exact solution\n",
    "t = GBM(So, mu, sig, W, T, N)[1]       # time increments for  plotting\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(t, soln)\n",
    "plt.ylabel('AAPL Stock Price, $')\n",
    "plt.title('Geometric Brownian Motion - AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Close = df['4. close']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_return(Close):\n",
    "    returns = []\n",
    "    for i in range(0, len(Close)-1):\n",
    "        today = Close[i+1]\n",
    "        yesterday = Close[i]\n",
    "        daily_return = (today - yesterday)/yesterday\n",
    "        returns.append(daily_return)\n",
    "    return returns\n",
    "\n",
    "returns = daily_return(Close)\n",
    "\n",
    "mu = np.mean(returns)*252.           # drift coefficient\n",
    "sig = np.std(returns)*np.sqrt(252.)  # diffusion coefficient\n",
    "\n",
    "print(mu, sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjusting the original time array \n",
    "time = np.linspace(1, len(Close), len(Close))\n",
    "time = [i/1780. for i in time]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(time, Close, label = 'Actual')\n",
    "plt.plot(t, soln, label = 'GBM (seed = 22)', ls='--')\n",
    "\n",
    "plt.ylabel('AAPL Stock Price, $')\n",
    "plt.title('Geometric Brownian Motion - AAPL')\n",
    "\n",
    "plt.legend(loc = 'upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1.\n",
    "N = 2.**6\n",
    "So = Close[0]\n",
    "\n",
    "seed = 5\n",
    "W = Brownian(seed, N)[0]\n",
    "soln2 = GBM(So, mu, sig, W, T, N)[0]    \n",
    "\n",
    "seed = 15\n",
    "W = Brownian(seed, N)[0]\n",
    "soln3 = GBM(So, mu, sig, W, T, N)[0]    \n",
    "\n",
    "# adjusting the original time array \n",
    "time = np.linspace(1, len(Close), len(Close))\n",
    "time = [i/1780. for i in time]\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(time, Close, label = 'Actual')\n",
    "plt.plot(t, soln, label = 'GBM (seed = 22)', ls = '--')\n",
    "plt.plot(t, soln2, label = 'GBM (seed = 5)', ls = '--')\n",
    "plt.plot(t, soln3, label = 'GBM (seed = 15)', ls = '--')\n",
    "\n",
    "plt.ylabel('AAPL Stock Price, $')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.title('Geometric Brownian Motion - AAPL')\n",
    "plt.title(\"AAPL BROWNIAN MOTION\")\n",
    "plt.savefig(\"Images/AAPL_BM_img.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Euler-Maruyama Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 1.\n",
    "N = 2.**6\n",
    "So = Close[0]\n",
    "\n",
    "seed = 5\n",
    "W = Brownian(seed, N)[0]\n",
    "soln2 = GBM(So, mu, sig, W, T, N)[0]    \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(t, soln2, label = 'GBM (seed = 5)', ls = '--')\n",
    "\n",
    "plt.ylabel('AAPL Stock Price, $')\n",
    "plt.title('Geometric Brownian Motion - AAPL')\n",
    "\n",
    "plt.legend(loc = 'upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact solution - GBM Model\n",
    "seed = 5\n",
    "So = Close[0]            # Initial Stock price \n",
    "W = Brownian(seed, N)[0]\n",
    "T = 1.\n",
    "N = 2.**6\n",
    "\n",
    "soln = GBM(So, mu, sig, W, T, N)[0]    # Exact solution\n",
    "t = GBM(So, mu, sig, W, T, N)[1]       # time increments for  plotting\n",
    "plt.plot(t, soln, label ='GBM')\n",
    "\n",
    "\n",
    "# Euler Maruyama Approximation\n",
    "def EM(So, mu, sigma, b, T, N, M):\n",
    "    dt = M * (1/N)  # EM step size\n",
    "    L = N / M\n",
    "    wi = [So]\n",
    "    for i in range(0,int(L)):\n",
    "        Winc = np.sum(b[(M*(i-1)+M):(M*i + M)])\n",
    "        w_i_new = wi[i]+mu*wi[i]*dt+sigma*wi[i]*Winc\n",
    "        wi.append(w_i_new)\n",
    "    return wi, dt\n",
    "\n",
    "\n",
    "# Changing the time step sizes\n",
    "#\n",
    "# dt = 0.03125\n",
    "b = Brownian(5, N)[1]    # Brownian increments \n",
    "M = 1                    \n",
    "L = N/M\n",
    "EM_approx_1 = EM(So, mu, sig, b, T, N, M)[0]\n",
    "time_EM_1 = np.linspace(0.,1.,L+1)\n",
    "\n",
    "# dt = 0.0625\n",
    "b = Brownian(5, N)[1]    # Brownian increments \n",
    "M = 4                    \n",
    "L = N/M\n",
    "EM_approx_2 = EM(So, mu, sig, b, T, N, M)[0]\n",
    "time_EM_2 = np.linspace(0.,1.,L+1)\n",
    "\n",
    "\n",
    "plt.plot(time_EM_1, EM_approx_1, label = 'dt = 0.03125', ls ='--')\n",
    "plt.plot(time_EM_2, EM_approx_2, label = 'dt = 0.0625', ls ='--')\n",
    "\n",
    "\n",
    "plt.ylabel('AAPL Stock Price, $')\n",
    "plt.title('Geometric Brownian Motion - AAPL')\n",
    "\n",
    "plt.legend(loc = 'upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " \"Forecasting at Scale\" (Abstract) \n",
    "\n",
    "Forecasting is a common data science task that helps organizations with capacity planning,  goal  setting,  and  anomaly  detection.   Despite  its  importance,  there  are serious  challenges  associated  with  producing  reliable  and  high  quality  forecasts  –especially  when  there  are  a  variety  of  time  series  and  analysts  with  expertise  intime  series  modeling  are  relatively  rare.   To  address  these  challenges,  we  describea  practical  approach  to  forecasting  “at  scale”  that  combines  configurable  models with  analyst-in-the-loop  performance  analysis.   We  propose  a  modular  regressionmodel  with  interpretable  parameters  that  can  be  intuitively  adjusted  by  analysts with  domain  knowledge  about  the  time  series.   We  describe  performance  analyses to compare and evaluate forecasting procedures, and automatically flag forecasts for manual review and adjustment.  Tools that help analysts to use their expertise most effectively enable reliable, practical forecasting of business time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the Data\n",
    "todays_day = date.today() \n",
    "start = datetime.datetime(2010, 1, 1)\n",
    "end = datetime.datetime(2019,7,9)\n",
    "\n",
    "\n",
    "aapl= pdr.get_data_yahoo(\"AAPL\", start=start, end=end)['Adj Close']\n",
    "\n",
    "aapl=pd.DataFrame(aapl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjustin the Data to the model\n",
    "aapl['Date'] = aapl.index\n",
    "aapl.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet requires columns ds (Date) and y (value)\n",
    "aapl = aapl.rename(columns={'Date': 'ds', 'Adj Close': 'y'})\n",
    "aapl.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put market cap in billions\n",
    "aapl['y'] = aapl['y'] / 1e9\n",
    "# Make the prophet model and fit on the data\n",
    "aapl_prophet = fbprophet.Prophet(changepoint_prior_scale=0.15)\n",
    "aapl_prophet.fit(aapl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a future dataframe for 2 years\n",
    "aapl_forecast = aapl_prophet.make_future_dataframe(periods=365 * 2, freq='D')\n",
    "# Make predictions\n",
    "aapl_forecast = aapl_prophet.predict(aapl_forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2 = model.plot_components(aapl_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "# horizon: the forecast horizon\n",
    "# initial: the size of the initial training period\n",
    "# period: the spacing between cutoff dates\n",
    "\n",
    "from fbprophet.diagnostics import cross_validation\n",
    "df_cv = cross_validation(model, initial='3200 days', period='90 days', horizon = '180 days')\n",
    "df_cv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = df_cv['cutoff'].unique()[0]\n",
    "df_cv = df_cv[df_cv['cutoff'] == cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.diagnostics import performance_metrics\n",
    "df_p = performance_metrics(df_cv)\n",
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = performance_metrics(df_cv)['mse'].mean()\n",
    "Prophet_MSE = df_p \n",
    "Prophet_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet.plot import plot_cross_validation_metric\n",
    "fig = plot_cross_validation_metric(df_cv, metric='rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_prophet.plot(aapl_forecast, xlabel = 'Date', ylabel = 'Adj Close');\n",
    "plt.title(\"AAPL PROPHET FORECAST\")\n",
    "plt.savefig(\"Images/AAPL_Prophet_img.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effect of Changepoint Prior Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 4 different changepoints\n",
    "for changepoint in [0.001, 0.05, 0.1, 0.5]:\n",
    "    model = fbprophet.Prophet(daily_seasonality=False, changepoint_prior_scale=changepoint)\n",
    "    model.fit(aapl)\n",
    "    \n",
    "    future = model.make_future_dataframe(periods=365, freq='D')\n",
    "    future = model.predict(future)\n",
    "    \n",
    "    aapl[changepoint] = future['yhat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Actual observations\n",
    "plt.plot(aapl['ds'], aapl['y'], 'ko', label = 'Observations')\n",
    "colors = {0.001: 'b', 0.05: 'r', 0.1: 'grey', 0.5: 'gold'}\n",
    "\n",
    "# Plot each of the changepoint predictions\n",
    "for changepoint in [0.001, 0.05, 0.1, 0.5]:\n",
    "    plt.plot(aapl['ds'], aapl[changepoint], color = colors[changepoint], label = '%.3f prior scale' % changepoint)\n",
    "    \n",
    "plt.legend(prop={'size': 14})\n",
    "plt.xlabel('Date'); plt.ylabel('Adj Close'); plt.title('Effect of Changepoint Prior Scale - AAPL Prophet');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_prophet.changepoints[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "TABLE=[LSTM_MSE,ARIMA_MSE, Prophet_MSE]\n",
    "Summary= pd.DataFrame(TABLE)\n",
    "Summary.rename(columns={0:\"MSE\"},inplace=True)\n",
    "models = [\"LSTM\",\"Arima\",\"Prophet\"]\n",
    "Summary[\"Models\"] = models\n",
    "Summary = Summary.set_index(\"Models\")\n",
    "Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Investment Portfolios"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.Markowitz's Minimum-Variance Portfolio (MVP).\n",
    "Markowitz’s depends on quadratic optimization of forecasted returns, frequently providing unstable and highly concentrated solutions.\n",
    "\n",
    "2.Traditional risk parity’s Inverse-Variance Portfolio (IVP).\n",
    "Traditional risk parity’s ignores useful covariance information.\n",
    "\n",
    "3.Hierarchical Risk Parity\n",
    "Drop forecasted returns and rely completely on covariance data, and \n",
    "Cluster assets based on correlation in order to allocate less weight to similar assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On 20151227 by MLdP <lopezdeprado@lbl.gov>\n",
    "# Hierarchical Risk Parity\n",
    "\n",
    "\n",
    "def getIVP(cov, **kargs):\n",
    "    # Compute the inverse-variance portfolio\n",
    "    ivp = 1. / np.diag(cov)\n",
    "    ivp /= ivp.sum()\n",
    "    return ivp\n",
    "\n",
    "\n",
    "def getClusterVar(cov,cItems):\n",
    "    # Compute variance per cluster\n",
    "    cov_=cov.loc[cItems,cItems] # matrix slice\n",
    "    w_=getIVP(cov_).reshape(-1,1)\n",
    "    cVar=np.dot(np.dot(w_.T,cov_),w_)[0,0]\n",
    "    return cVar\n",
    "\n",
    "\n",
    "def getQuasiDiag(link):\n",
    "    # Sort clustered items by distance\n",
    "    link = link.astype(int)\n",
    "    sortIx = pd.Series([link[-1, 0], link[-1, 1]])\n",
    "    numItems = link[-1, 3]  # number of original items\n",
    "    while sortIx.max() >= numItems:\n",
    "        sortIx.index = range(0, sortIx.shape[0] * 2, 2)  # make space\n",
    "        df0 = sortIx[sortIx >= numItems]  # find clusters\n",
    "        i = df0.index\n",
    "        j = df0.values - numItems\n",
    "        sortIx[i] = link[j, 0]  # item 1\n",
    "        df0 = pd.Series(link[j, 1], index=i + 1)\n",
    "        sortIx = sortIx.append(df0)  # item 2\n",
    "        sortIx = sortIx.sort_index()  # re-sort\n",
    "        sortIx.index = range(sortIx.shape[0])  # re-index\n",
    "    return sortIx.tolist()\n",
    "\n",
    "\n",
    "def getRecBipart(cov, sortIx):\n",
    "    # Compute HRP alloc\n",
    "    w = pd.Series(1, index=sortIx)\n",
    "    cItems = [sortIx]  # initialize all items in one cluster\n",
    "    while len(cItems) > 0:\n",
    "        cItems = [i[j:k] for i in cItems for j, k in ((0, len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]  # bi-section\n",
    "        for i in range(0, len(cItems), 2):  # parse in pairs\n",
    "            cItems0 = cItems[i]  # cluster 1\n",
    "            cItems1 = cItems[i + 1]  # cluster 2\n",
    "            cVar0 = getClusterVar(cov, cItems0)\n",
    "            cVar1 = getClusterVar(cov, cItems1)\n",
    "            alpha = 1 - cVar0 / (cVar0 + cVar1)\n",
    "            w[cItems0] *= alpha  # weight 1\n",
    "            w[cItems1] *= 1 - alpha  # weight 2\n",
    "    return w\n",
    "\n",
    "\n",
    "def correlDist(corr):\n",
    "    # A distance matrix based on correlation, where 0<=d[i,j]<=1\n",
    "    # This is a proper distance metric\n",
    "    dist = ((1 - corr) / 2.)**.5  # distance matrix\n",
    "    return dist\n",
    "\n",
    "\n",
    "def getHRP(cov, corr):\n",
    "    # Construct a hierarchical portfolio\n",
    "    dist = correlDist(corr)\n",
    "    link = sch.linkage(dist, 'single')\n",
    "    #dn = sch.dendrogram(link, labels=cov.index.values, label_rotation=90)\n",
    "    #plt.show()\n",
    "    sortIx = getQuasiDiag(link)\n",
    "    sortIx = corr.index[sortIx].tolist()\n",
    "    hrp = getRecBipart(cov, sortIx)\n",
    "    return hrp.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMVP(cov):\n",
    "\n",
    "    cov = cov.T.values\n",
    "    n = len(cov)\n",
    "    N = 100\n",
    "    mus = [10 ** (5.0 * t / N - 1.0) for t in range(N)]\n",
    "\n",
    "    # Convert to cvxopt matrices\n",
    "    S = opt.matrix(cov)\n",
    "    #pbar = opt.matrix(np.mean(returns, axis=1))\n",
    "    pbar = opt.matrix(np.ones(cov.shape[0]))\n",
    "\n",
    "    # Create constraint matrices\n",
    "    G = -opt.matrix(np.eye(n))  # negative n x n identity matrix\n",
    "    h = opt.matrix(0.0, (n, 1))\n",
    "    A = opt.matrix(1.0, (1, n))\n",
    "    b = opt.matrix(1.0)\n",
    "\n",
    "    # Calculate efficient frontier weights using quadratic programming\n",
    "    portfolios = [solvers.qp(mu * S, -pbar, G, h, A, b)['x']\n",
    "                  for mu in mus]\n",
    "    ## CALCULATE RISKS AND RETURNS FOR FRONTIER\n",
    "    returns = [blas.dot(pbar, x) for x in portfolios]\n",
    "    risks = [np.sqrt(blas.dot(x, S * x)) for x in portfolios]\n",
    "    ## CALCULATE THE 2ND DEGREE POLYNOMIAL OF THE FRONTIER CURVE\n",
    "    m1 = np.polyfit(returns, risks, 2)\n",
    "    x1 = np.sqrt(m1[2] / m1[0])\n",
    "    # CALCULATE THE OPTIMAL PORTFOLIO\n",
    "    wt = solvers.qp(opt.matrix(x1 * S), -pbar, G, h, A, b)['x']\n",
    "\n",
    "    return list(wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_portfolios(returns):\n",
    "    \n",
    "    cov, corr = returns.cov(), returns.corr()\n",
    "    hrp = getHRP(cov, corr)\n",
    "    ivp = getIVP(cov)\n",
    "    ivp = pd.Series(ivp, index=cov.index)\n",
    "    mvp = getMVP(cov)\n",
    "    mvp = pd.Series(mvp, index=cov.index)\n",
    "    \n",
    "    portfolios = pd.DataFrame([mvp, ivp, hrp], index=['MVP', 'IVP', 'HRP']).T\n",
    "    \n",
    "    return portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = {\n",
    "    \"Apple\": \"AAPL\",\n",
    "    \"Amazon\": \"AMZN\",\n",
    "    \"Exxon Mobil\": \"XOM\",\n",
    "    \"Microsoft\": \"MSFT\",\n",
    "    \"JP Morgan\": \"JPM\"\n",
    "#    # \"Diversified Fixed Income Portfolio\":\"DFXIX\"\n",
    "#     \"STANDARD POORS\":\"SPX\"\n",
    "}\n",
    "stock_df = pd.DataFrame()\n",
    "stock_symbols = list(stocks.values())# stocks = pd.DataFrame(list(stocks.items()), columns=[\"name\", \"symbol\"])\n",
    "ts = TimeSeries(key=\"VW8RXL88BTZWTRN4\",output_format='pandas')\n",
    "symbols = []\n",
    "for symbol in stock_symbols:\n",
    "    symbols.append(symbol)\n",
    "    print(symbol)\n",
    "    data, _ = ts.get_daily(symbol=symbol, outputsize='full')\n",
    "    close = data['4. close']\n",
    "    stock_df = pd.concat([stock_df, close], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.columns = symbols\n",
    "stock_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_stock_df = stock_df[\"2010-01-01\":\"2019-07-05\"]\n",
    "returns = range_stock_df.to_returns().dropna()\n",
    "portfolios = get_all_portfolios(returns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
